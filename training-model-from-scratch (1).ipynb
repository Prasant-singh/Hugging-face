{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:15:40.738022Z","iopub.execute_input":"2025-08-29T06:15:40.738361Z","iopub.status.idle":"2025-08-29T06:15:52.354971Z","shell.execute_reply.started":"2025-08-29T06:15:40.738338Z","shell.execute_reply":"2025-08-29T06:15:52.350697Z"}},"outputs":[{"name":"stdout","text":"Collecting datasets\n  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting fsspec[http]<=2025.3.0,>=2023.1.0\n  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (2.0.2)\nCollecting multiprocess<0.70.17\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (20.0.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.3.0)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.67.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets) (2.32.4)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.33.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.18.0)\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n  Downloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nInstalling collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.4.0\n    Uninstalling dill-0.4.0:\n      Successfully uninstalled dill-0.4.0\nSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 xxhash-3.5.0 yarl-1.20.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## GETTING DATA\n","metadata":{}},{"cell_type":"code","source":"def any_keyword_in_string(string,keywords):\n    for keyword in keywords:\n        if keyword in string:\n            return True \n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:15:52.357925Z","iopub.execute_input":"2025-08-29T06:15:52.358177Z","iopub.status.idle":"2025-08-29T06:15:52.369876Z","shell.execute_reply.started":"2025-08-29T06:15:52.358152Z","shell.execute_reply":"2025-08-29T06:15:52.363572Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:15:52.371451Z","iopub.execute_input":"2025-08-29T06:15:52.371708Z","iopub.status.idle":"2025-08-29T06:15:52.394688Z","shell.execute_reply.started":"2025-08-29T06:15:52.371685Z","shell.execute_reply":"2025-08-29T06:15:52.389286Z"}},"outputs":[{"name":"stdout","text":"False True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from collections import defaultdict\nfrom tqdm import tqdm\nfrom datasets import Dataset\n\n\ndef filter_streaming_dataset(dataset, filters):\n    filtered_dict = defaultdict(list)\n    total = 0\n    for sample in tqdm(iter(dataset)):\n        total += 1\n        if any_keyword_in_string(sample[\"content\"], filters):\n            for k, v in sample.items():\n                filtered_dict[k].append(v)\n    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n    return Dataset.from_dict(filtered_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:15:52.395272Z","iopub.execute_input":"2025-08-29T06:15:52.395466Z","iopub.status.idle":"2025-08-29T06:15:53.641167Z","shell.execute_reply.started":"2025-08-29T06:15:52.395445Z","shell.execute_reply":"2025-08-29T06:15:53.635923Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# takes a very long time to execute\n# from datasets import load_dataset\n\n# split = \"train\" \n# filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n\n# data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n# filtered_data = filter_streaming_dataset(data, filters)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:15:53.647925Z","iopub.execute_input":"2025-08-29T06:15:53.648131Z","iopub.status.idle":"2025-08-29T06:15:53.657933Z","shell.execute_reply.started":"2025-08-29T06:15:53.648111Z","shell.execute_reply":"2025-08-29T06:15:53.652446Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\nds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\nds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n\nraw_datasets = DatasetDict(\n    {\n        \"train\": ds_train, \n        \"valid\": ds_valid,  \n    }\n)\n\nraw_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:15:53.659276Z","iopub.execute_input":"2025-08-29T06:15:53.659614Z","iopub.status.idle":"2025-08-29T06:16:34.921779Z","shell.execute_reply.started":"2025-08-29T06:15:53.659593Z","shell.execute_reply":"2025-08-29T06:16:34.915863Z"}},"outputs":[{"name":"stderr","text":"Generating train split: 606720 examples [00:21, 27917.93 examples/s]\nGenerating validation split: 100%|██████████| 3322/3322 [00:00<00:00, 25172.49 examples/s]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 606720\n    })\n    valid: Dataset({\n        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n        num_rows: 3322\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"for key in raw_datasets[\"train\"][0]:\n    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:16:34.922871Z","iopub.execute_input":"2025-08-29T06:16:34.923234Z","iopub.status.idle":"2025-08-29T06:16:34.933932Z","shell.execute_reply.started":"2025-08-29T06:16:34.923209Z","shell.execute_reply":"2025-08-29T06:16:34.929633Z"}},"outputs":[{"name":"stdout","text":"REPO_NAME: kmike/scikit-learn\nPATH: sklearn/utils/__init__.py\nCOPIES: 3\nSIZE: 10094\nCONTENT: \"\"\"\nThe :mod:`sklearn.utils` module includes various utilites.\n\"\"\"\n\nfrom collections import Sequence\n\nimport numpy as np\nfrom scipy.sparse import issparse\nimport warnings\n\nfrom .murmurhash import murm\nLICENSE: bsd-3-clause\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# PREPARING DATA","metadata":{}},{"cell_type":"code","source":"def tokenize(element):\n    outputs = tokenizer(\n        element[\"content\"],\n        truncation=True,\n        max_length=context_length,\n        return_overflowing_tokens=True,\n        return_length=True,\n    )\n    input_batch = []\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n        if length == context_length:\n            input_batch.append(input_ids)\n    return {\"input_ids\": input_batch}\n\n\ntokenized_datasets = raw_datasets.map(\n    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n)\ntokenized_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:19:43.297423Z","iopub.execute_input":"2025-08-29T06:19:43.297801Z","iopub.status.idle":"2025-08-29T06:57:37.686430Z","shell.execute_reply.started":"2025-08-29T06:19:43.297772Z","shell.execute_reply":"2025-08-29T06:57:37.679729Z"}},"outputs":[{"name":"stderr","text":"Map: 100%|██████████| 606720/606720 [37:40<00:00, 268.36 examples/s]\nMap: 100%|██████████| 3322/3322 [00:13<00:00, 247.10 examples/s]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 16702061\n    })\n    valid: Dataset({\n        features: ['input_ids'],\n        num_rows: 93164\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initializing a New Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n\nconfig = AutoConfig.from_pretrained(\n    \"gpt2\",\n    vocab_size=len(tokenizer),\n    n_ctx=context_length,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T07:17:34.353845Z","iopub.execute_input":"2025-08-29T07:17:34.354247Z","iopub.status.idle":"2025-08-29T07:17:34.474091Z","shell.execute_reply.started":"2025-08-29T07:17:34.354217Z","shell.execute_reply":"2025-08-29T07:17:34.469949Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, GPT2LMHeadModel, AutoConfig\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m),\n\u001b[1;32m      6\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39mcontext_length,\n\u001b[1;32m      7\u001b[0m     bos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbos_token_id,\n\u001b[1;32m      8\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m      9\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"model = GPT2LMHeadModel(config)\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T07:17:30.689313Z","iopub.execute_input":"2025-08-29T07:17:30.689720Z","iopub.status.idle":"2025-08-29T07:17:30.736328Z","shell.execute_reply.started":"2025-08-29T07:17:30.689689Z","shell.execute_reply":"2025-08-29T07:17:30.732501Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m(config)\n\u001b[1;32m      2\u001b[0m model_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT-2 size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'GPT2LMHeadModel' is not defined"],"ename":"NameError","evalue":"name 'GPT2LMHeadModel' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:59:01.825686Z","iopub.execute_input":"2025-08-29T06:59:01.826057Z","iopub.status.idle":"2025-08-29T06:59:01.835956Z","shell.execute_reply.started":"2025-08-29T06:59:01.826027Z","shell.execute_reply":"2025-08-29T06:59:01.831905Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"codeparrot-ds\",\n    per_device_train_batch_size=64,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=32,\n    # evaluation_strategy=\"steps\",\n    eval_steps=5_000,\n    logging_steps=5_000,\n    # gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    weight_decay=0.1,\n    warmup_steps=1_000,\n    lr_scheduler_type=\"cosine\",\n    learning_rate=5e-4,\n    save_steps=5_000,\n    fp16=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    processing_class=tokenizer,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"valid\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T07:17:25.096037Z","iopub.execute_input":"2025-08-29T07:17:25.096433Z","iopub.status.idle":"2025-08-29T07:17:25.148571Z","shell.execute_reply.started":"2025-08-29T07:17:25.096401Z","shell.execute_reply":"2025-08-29T07:17:25.142777Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodeparrot-ds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m---> 22\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     23\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     25\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     26\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     27\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     28\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:58:06.967999Z","iopub.status.idle":"2025-08-29T06:58:06.968433Z","shell.execute_reply.started":"2025-08-29T06:58:06.968126Z","shell.execute_reply":"2025-08-29T06:58:06.968138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\noutput_dir = \"./Model_final\"\n\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer have been saved to {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOADING AND VERIFYING SAVED MODEL","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n\n# Define the path to your saved model\noutput_dir = \"./Model_final\"\n\n# Load the fine-tuned model and tokenizer from your saved directory\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel = AutoModelForCausalLM.from_pretrained(output_dir)\n\n# Create a text-generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# A prompt relevant to your fine-tuning data\nprompt = \"import pandas as pd\\n\\n# Create a new DataFrame from a dictionary\"\n\n# Generate code\nresult = pipe(prompt, max_length=100, num_return_sequences=1)\n\n# Print the generated code\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\n# Assuming 'trainer' is the same Trainer object you used for training\n# It already has the model, tokenizer, and eval_dataset loaded.\neval_results = trainer.evaluate()\n\n# The 'evaluate' method returns a dictionary with metrics.\n# We are interested in 'eval_loss'.\nprint(f\"Evaluation results: {eval_results}\")\n\n# Calculate perplexity\nperplexity = math.exp(eval_results['eval_loss'])\nprint(f\"Perplexity: {perplexity:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}